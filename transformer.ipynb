{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T07:03:31.082598Z","iopub.execute_input":"2025-12-18T07:03:31.083056Z","iopub.status.idle":"2025-12-18T07:03:35.718288Z","shell.execute_reply.started":"2025-12-18T07:03:31.083019Z","shell.execute_reply":"2025-12-18T07:03:35.717096Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 500):\n        super().__init__()\n\n        #PE(pos, 2i) = sin(pos / 1000^(2i/d_model))\n        #PE(pos, 2i+1) = cos(pos / 1000^(2i/d_model))\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position*div_term)\n        pe[:, 1::2] = torch.cos(position*div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1), :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T07:14:41.779060Z","iopub.execute_input":"2025-12-18T07:14:41.780282Z","iopub.status.idle":"2025-12-18T07:14:41.788294Z","shell.execute_reply.started":"2025-12-18T07:14:41.780241Z","shell.execute_reply":"2025-12-18T07:14:41.786849Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int):\n        super().__init__()\n        assert d_model % num_heads == 0\n        self.d_k = d_model // num_heads\n        self.num_heads = num_heads\n        \n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_o = nn.Linear(d_model, d_model)\n        \n        self.dropout = nn.Dropout(0.1)\n    \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        attn_probs = F.softmax(attn_scores, dim=-1)\n        attn_probs = self.dropout(attn_probs)\n        return torch.matmul(attn_probs, V)\n    \n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n        \n        Q = self.w_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.w_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.w_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n        \n        return self.w_o(attn_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T07:20:35.170864Z","iopub.execute_input":"2025-12-18T07:20:35.171226Z","iopub.status.idle":"2025-12-18T07:20:35.183111Z","shell.execute_reply.started":"2025-12-18T07:20:35.171204Z","shell.execute_reply":"2025-12-18T07:20:35.182075Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout=0.1): \n        #d_ff: hidden layer dimension \n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff) #input layer\n        self.linear2 = nn.Linear(d_ff, d_model) #hidden layer\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T07:20:36.381407Z","iopub.execute_input":"2025-12-18T07:20:36.381945Z","iopub.status.idle":"2025-12-18T07:20:36.389900Z","shell.execute_reply.started":"2025-12-18T07:20:36.381909Z","shell.execute_reply":"2025-12-18T07:20:36.388201Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model: int, num_heads: int , d_ff: int, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.ff = FeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.ff(x)\n        return self.norm2(x + self.dropout(ff_output))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T07:20:37.728150Z","iopub.execute_input":"2025-12-18T07:20:37.728527Z","iopub.status.idle":"2025-12-18T07:20:37.735653Z","shell.execute_reply.started":"2025-12-18T07:20:37.728502Z","shell.execute_reply":"2025-12-18T07:20:37.734573Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout=0.1):\n        super().__init__()\n        self.masked_attn = MultiHeadAttention(d_model, num_heads)\n        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n        attn_output = self.masked_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        attn_output = self.enc_dec_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        ff_output = self.ff(x)\n        return self.norm3(x + self.dropout(ff_output))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T07:20:39.472599Z","iopub.execute_input":"2025-12-18T07:20:39.473031Z","iopub.status.idle":"2025-12-18T07:20:39.482753Z","shell.execute_reply.started":"2025-12-18T07:20:39.472994Z","shell.execute_reply":"2025-12-18T07:20:39.481060Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, d_model: int = 512, num_heads: int = 8,\n                num_layers: int = 6, d_ff: int = 2048, max_len: int = 5000, dropout: float = 0.1):\n        super().__init__()\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n\n        self.final_linear = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def create_padding_mask(self, seq, pad_token=0):\n        return (seq != pad_token).unsqueeze(1).unsqueeze(2)\n\n    def create_look_ahead_mask(self, seq_len):\n        mask = (torch.triu(torch.ones(seq_len, seq_len), diagonal=1)).bool()\n        return ~mask\n\n    def forward(self, src, tgt, src_pad_mask=None, tgt_pad_mask=None):\n        if src_pad_mask is None:\n            src_pad_mask = self.create_padding_mask(src)\n        if tgt_pad_mask is None:\n            tgt_pad_mask = self.create_padding_mask(tgt)\n\n        tgt_look_ahead_mask = self.create_look_ahead_mask(tgt.size(1)).to(tgt.device)\n        tgt_mask = tgt_pad_mask & tgt_look_ahead_mask\n\n        src_emb = self.dropout(self.pos_encoding(self.src_embedding(src) * math.sqrt(self.src_embedding.embedding_dim)))\n        tgt_emb = self.dropout(self.pos_encoding(self.tgt_embedding(tgt) * math.sqrt(self.tgt_embedding.embedding_dim)))\n\n        enc_output = src_emb\n        for layer in self.encoder_layers:\n            enc_output = layer(enc_output, src_pad_mask)\n\n        dec_output = tgt_emb\n        for layer in self.decoder_layers:\n            dec_output = layer(dec_output, enc_output, src_pad_mask, tgt_mask)\n\n        return self.final_linear(dec_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T07:20:41.109234Z","iopub.execute_input":"2025-12-18T07:20:41.109665Z","iopub.status.idle":"2025-12-18T07:20:41.127268Z","shell.execute_reply.started":"2025-12-18T07:20:41.109637Z","shell.execute_reply":"2025-12-18T07:20:41.126132Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"src_texts = [\n    \"The Transformer is a powerful deep learning architecture.\",\n    \"It was introduced in the paper Attention is All You Need.\"\n]\n\ntgt_texts = [\n    \"Transformer là một kiến trúc học sâu mạnh mẽ.\",\n    \"Nó được giới thiệu trong bài báo Attention is All You Need.\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T07:20:43.057487Z","iopub.execute_input":"2025-12-18T07:20:43.057969Z","iopub.status.idle":"2025-12-18T07:20:43.064157Z","shell.execute_reply.started":"2025-12-18T07:20:43.057935Z","shell.execute_reply":"2025-12-18T07:20:43.062712Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\n\nBOS_TOKEN = \"<bos>\"\nEOS_TOKEN = \"<eos>\"\nPAD_TOKEN = \"<pad>\"\nUNK_TOKEN = \"<unk>\"\n\ndef build_vocab(texts, min_freq=1):\n    vocab = {PAD_TOKEN: 0, BOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n    idx = 4\n    for text in texts:\n        for word in text.lower().split():\n            if word not in vocab:\n                vocab[word] = idx\n                idx += 1\n    return vocab\n\nsrc_vocab = build_vocab(src_texts)\ntgt_vocab = build_vocab(tgt_texts)\n\nsrc_vocab_size = len(src_vocab)\ntgt_vocab_size = len(tgt_vocab)\n\ndef text_to_indices(text, vocab, is_src=True):\n    words = text.lower().split() if is_src else text.split()\n    return [vocab.get(BOS_TOKEN, 1)] + [vocab.get(w, vocab[UNK_TOKEN]) for w in words] + [vocab.get(EOS_TOKEN, 2)]\n\nclass TranslationDataset(Dataset):\n    def __init__(self, src_texts, tgt_texts):\n        self.src = [torch.tensor(text_to_indices(s, src_vocab)) for s in src_texts]\n        self.tgt = [torch.tensor(text_to_indices(t, tgt_vocab, is_src=False)) for t in tgt_texts]\n    \n    def __len__(self):\n        return len(self.src)\n    \n    def __getitem__(self, idx):\n        return self.src[idx], self.tgt[idx]\n\ndataset = TranslationDataset(src_texts, tgt_texts)\n\ndef collate_fn(batch):\n    src_batch, tgt_batch = zip(*batch)\n    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, padding_value=src_vocab[PAD_TOKEN], batch_first=True)\n    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_batch, padding_value=tgt_vocab[PAD_TOKEN], batch_first=True)\n\n    tgt_input = tgt_padded[:, :-1]\n    tgt_target = tgt_padded[:, 1:]\n    return src_padded, tgt_input, tgt_target\n\ndataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T07:20:48.060664Z","iopub.execute_input":"2025-12-18T07:20:48.061148Z","iopub.status.idle":"2025-12-18T07:20:48.076236Z","shell.execute_reply.started":"2025-12-18T07:20:48.061111Z","shell.execute_reply":"2025-12-18T07:20:48.074858Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"model = Transformer(\n    src_vocab_size=src_vocab_size,\n    tgt_vocab_size=tgt_vocab_size,\n    d_model=128,\n    num_heads=4,\n    num_layers=2,\n    d_ff=256,\n    max_len=100,\n    dropout=0.1\n)\n\nmodel.train()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab[PAD_TOKEN])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T07:20:49.811767Z","iopub.execute_input":"2025-12-18T07:20:49.812224Z","iopub.status.idle":"2025-12-18T07:20:49.836064Z","shell.execute_reply.started":"2025-12-18T07:20:49.812195Z","shell.execute_reply":"2025-12-18T07:20:49.834929Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"epochs = 1000  \n\nfor epoch in range(epochs):\n    total_loss = 0\n    for src, tgt_input, tgt_target in dataloader:\n        optimizer.zero_grad()\n        \n        output = model(src, tgt_input) \n        \n        loss = criterion(\n            output.reshape(-1, tgt_vocab_size),\n            tgt_target.reshape(-1)\n        )\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}, Loss: {total_loss / len(dataloader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T07:22:29.070131Z","iopub.execute_input":"2025-12-18T07:22:29.070610Z","iopub.status.idle":"2025-12-18T07:22:55.362334Z","shell.execute_reply.started":"2025-12-18T07:22:29.070575Z","shell.execute_reply":"2025-12-18T07:22:55.361222Z"}},"outputs":[{"name":"stdout","text":"Epoch 0, Loss: 3.2813\nEpoch 100, Loss: 0.0907\nEpoch 200, Loss: 0.0616\nEpoch 300, Loss: 0.0962\nEpoch 400, Loss: 0.0565\nEpoch 500, Loss: 0.0138\nEpoch 600, Loss: 0.1300\nEpoch 700, Loss: 0.0588\nEpoch 800, Loss: 0.0319\nEpoch 900, Loss: 0.0705\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T07:24:10.111887Z","iopub.execute_input":"2025-12-18T07:24:10.112263Z","iopub.status.idle":"2025-12-18T07:24:10.120343Z","shell.execute_reply.started":"2025-12-18T07:24:10.112237Z","shell.execute_reply":"2025-12-18T07:24:10.119498Z"}},"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"Transformer(\n  (src_embedding): Embedding(21, 128)\n  (tgt_embedding): Embedding(25, 128)\n  (pos_encoding): PositionalEncoding()\n  (encoder_layers): ModuleList(\n    (0-1): 2 x EncoderLayer(\n      (self_attn): MultiHeadAttention(\n        (w_q): Linear(in_features=128, out_features=128, bias=True)\n        (w_k): Linear(in_features=128, out_features=128, bias=True)\n        (w_v): Linear(in_features=128, out_features=128, bias=True)\n        (w_o): Linear(in_features=128, out_features=128, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (linear1): Linear(in_features=128, out_features=256, bias=True)\n        (linear2): Linear(in_features=256, out_features=128, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (decoder_layers): ModuleList(\n    (0-1): 2 x DecoderLayer(\n      (masked_attn): MultiHeadAttention(\n        (w_q): Linear(in_features=128, out_features=128, bias=True)\n        (w_k): Linear(in_features=128, out_features=128, bias=True)\n        (w_v): Linear(in_features=128, out_features=128, bias=True)\n        (w_o): Linear(in_features=128, out_features=128, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (enc_dec_attn): MultiHeadAttention(\n        (w_q): Linear(in_features=128, out_features=128, bias=True)\n        (w_k): Linear(in_features=128, out_features=128, bias=True)\n        (w_v): Linear(in_features=128, out_features=128, bias=True)\n        (w_o): Linear(in_features=128, out_features=128, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ff): FeedForward(\n        (linear1): Linear(in_features=128, out_features=256, bias=True)\n        (linear2): Linear(in_features=256, out_features=128, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (final_linear): Linear(in_features=128, out_features=25, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}],"execution_count":75},{"cell_type":"code","source":"def translate(sentence):\n    src_indices = torch.tensor([text_to_indices(sentence, src_vocab, is_src=True)]).to(next(model.parameters()).device)\n    \n    \n    tgt_indices = torch.tensor([[tgt_vocab[BOS_TOKEN]]])\n    \n    for _ in range(50):  \n        tgt_input = tgt_indices\n        with torch.no_grad():\n            output = model(src_indices, tgt_input)\n        next_token = output[:, -1, :].argmax(dim=-1)\n        tgt_indices = torch.cat([tgt_indices, next_token.unsqueeze(1)], dim=1)\n        \n        if next_token.item() == tgt_vocab[EOS_TOKEN]:\n            break\n    \n    \n    inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n    words = [inv_tgt_vocab.get(i.item(), \"\") for i in tgt_indices[0, 1:]]  \n    return \" \".join(words).replace(\" <eos>\", \"\")\n\nprint(translate(\"The Transformer is a powerful deep learning architecture.\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T07:24:15.622324Z","iopub.execute_input":"2025-12-18T07:24:15.623221Z","iopub.status.idle":"2025-12-18T07:24:15.680243Z","shell.execute_reply.started":"2025-12-18T07:24:15.623180Z","shell.execute_reply":"2025-12-18T07:24:15.679153Z"}},"outputs":[{"name":"stdout","text":"<unk> là một kiến trúc học sâu mạnh mẽ.\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}