{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>1. Create Encoder and Decoder Layers</h1>","metadata":{}},{"cell_type":"code","source":"#Import important libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-19T01:36:10.763425Z","iopub.execute_input":"2025-12-19T01:36:10.763686Z","iopub.status.idle":"2025-12-19T01:36:14.573789Z","shell.execute_reply.started":"2025-12-19T01:36:10.763667Z","shell.execute_reply":"2025-12-19T01:36:14.573049Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"\n## 1.1 Create Positional Encoding\n\n<span style=\"font-size:18px;\">\n\n###  Definition\n\n<strong>Positional Encoding</strong> is a technique used in Transformer models to inject information about the position of each token in a sequence into its embedding.\n\nSince Transformers do not process data sequentially (unlike RNNs), they have **no inherent sense of word order**. Positional encoding solves this by allowing the model to distinguish between tokens at different positions and capture sentence structure.\n\nWithout positional encoding, a Transformer would struggle to process sequential data effectively.\n\n---\n\n###  Example of Positional Encoding\n\nSuppose we have a Transformer model that translates English sentences into French.\n\nConsider the sentence:\n\n> **\"The cat sat on the mat.\"**\n\n#### Step 1: Tokenization\n\nThe sentence is first tokenized into individual tokens:\n\n```text\n[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n```\n\n#### Step 2: Word Embeddings\n\nEach token is then mapped to a high-dimensional vector through an embedding layer. These embeddings capture **semantic meaning**, but **do not encode word order**.\n\n```text\nEmbeddings = {E₁, E₂, E₃, E₄, E₅, E₆}\n```\n\nwhere each embedding $E_i$ is a 4-dimensional vector.\n\n#### Step 3: Adding Positional Encoding\n\nTo provide the model with information about token positions, **positional encodings** are added to the word embeddings:\n\n```text\nFinal Input = Word Embedding + Positional Encoding\n```\n\nThis ensures that each token has a **unique representation based on both meaning and position**, allowing the model to understand word order.\n\n---\n\n###  Positional Encoding Formula\n\nThe original Transformer paper uses **sinusoidal positional encodings**, defined as:\n\n$$\nPE_{(pos,\\,2i)} = \\sin\\left(\\frac{pos}{10000^{2i / d_{model}}}\\right)\n$$\n\n$$\nPE_{(pos,\\,2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i / d_{model}}}\\right)\n$$\n\nwhere:\n\n- $pos$ : position of the token in the sequence  \n- $i$ : dimension index  \n- $d_{model}$ : embedding dimension  \n\nThis formulation allows the model to generalize to sequence lengths longer than those seen during training.\n</span>","metadata":{}},{"cell_type":"code","source":"#Implementation of Positional Encoding\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 500):\n        super().__init__()\n\n        #PE(pos, 2i) = sin(pos / 1000^(2i/d_model))\n        #PE(pos, 2i+1) = cos(pos / 1000^(2i/d_model))\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position*div_term) #even index\n        pe[:, 1::2] = torch.cos(position*div_term) #odd index\n        pe = pe.unsqueeze(0) #add dimension to index 0 \n        self.register_buffer('pe', pe)\n        \n    #Word Embedding + Positional Encoding\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1), :] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T01:36:18.639705Z","iopub.execute_input":"2025-12-19T01:36:18.640411Z","iopub.status.idle":"2025-12-19T01:36:18.646370Z","shell.execute_reply.started":"2025-12-19T01:36:18.640381Z","shell.execute_reply":"2025-12-19T01:36:18.645641Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## 1.2 Create Multi Head Attention\n\n<span style=\"font-size:18px;\">\n\n### Understanding self-attention mechanism\n\nBefore diving into multi-head attention, let’s first understand the standard <strong>self-attention mechanism</strong>, also known as <strong>scaled dot-product attention</strong>.\n\nGiven a set of input vectors, self-attention computes attention scores to determine how much focus each element in the sequence should have on the others. This is done using three key matrices:\n\n- Query (Q): Represents the current word's relationship with others.\n- Key (K): Represents the words that are being compared against.\n- Value (V): Contains the actual word representations.\n\n<br><br>\n\n<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20231212180658/selfattne.png\" style=\"display: block; margin: 0 auto;\" width=\"600\">\n\n<br>\n<br>\n<br>\n$$\n\\begin{aligned}\n\\text{Attention}(Q, K, V) &= softmax\\left( \\frac{Q K^{T}}{\\sqrt{d_k}} \\right) V\n\\end{aligned}\n$$\n<br>\n\n### What is Multi-Head Attention?\nMulti-head attention extends self-attention by splitting the input into multiple heads, enabling the model to capture diverse relationships and patterns.\n\nInstead of using a single set of $Q$, $K$, $V$ matrices, the input embeddings are projected into multiple sets (heads), each with its own $Q$, $K$, $V$:\n\n1. **Linear Transformation**: The input $X$ is projected into multiple smaller-dimensional subspaces using different weight matrices.\n\n$$  \n\\mathbf{Q}_i = \\mathbf{X} \\mathbf{W}^Q_i, \\quad \n\\mathbf{K}_i = \\mathbf{X} \\mathbf{W}^K_i, \\quad \n\\mathbf{V}_i = \\mathbf{X} \\mathbf{W}^V_i\n  $$\n\nwhere $i$ denotes the head index.\nIndependent Attention Computation: Each head independently computes its own self-attention using the scaled dot-product formula.\nConcatenation: The outputs from all heads are concatenated.\nFinal Linear Transformation: A final weight matrix is applied to transform the concatenated output into the desired dimension.\n\n2. **Independent Attention Computation**: Each head independently computes its own self-attention using the scaled dot-product formula.\n3. **Concatenation**: The outputs from all heads are concatenated.\n4. **Final Linear Transformation**: A final weight matrix is applied to transform the concatenated output into the desired dimension.\n\n<br>\n<br>\n<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20231212181418/multihead.png\"style=\"display: block; margin: 0 auto;\" width=\"600\">\n<br>\n<br>\n\nMathematically, multi-head attention is expressed as:\n$$\n    MultiHead(Q, K, V) = Concat(head_1, head_2,...,head_h) \\mathbf{W}^O\n$$\n$where$\n$$\n    head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n$$\n\n$W^O$ is a final weight matrix to project the concatenated output back into the model's required dimensions.\n\n### Why use multi attention head?\n\nMulti-head attention provides several advantages:\n\n- **Captures different relationship**: Different heads attend to different aspects of the input.\n- **Improves learning efficiency**: By operating in parallel, multiple heads allow for better learning of dependencies.\n- **Enhances robustness**: The model doesn’t rely on a single attention pattern, reducing overfitting.\n\n</span>","metadata":{}},{"cell_type":"code","source":"#Implementation of MultiHeadAttention\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int):\n        super().__init__()\n        assert d_model % num_heads == 0\n        self.d_k = d_model // num_heads\n        self.num_heads = num_heads\n        \n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_o = nn.Linear(d_model, d_model)\n        \n        self.dropout = nn.Dropout(0.1)\n    \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        attn_probs = F.softmax(attn_scores, dim=-1)\n        attn_probs = self.dropout(attn_probs)\n        return torch.matmul(attn_probs, V)\n    \n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n        \n        Q = self.w_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.w_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.w_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n        \n        return self.w_o(attn_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T01:36:23.728477Z","iopub.execute_input":"2025-12-19T01:36:23.728761Z","iopub.status.idle":"2025-12-19T01:36:23.737997Z","shell.execute_reply.started":"2025-12-19T01:36:23.728740Z","shell.execute_reply":"2025-12-19T01:36:23.736866Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## 1.3 Create Feed Forward Network\n\n<span style=\"font-size:18px;\">\n\n### Key characteristic of FNN:\n1. **Fully connected layers**:\n   <br>\n    The FFN comprises two linear (fully connected) layers that transform the input data. The first layer expands the input dimension from $dmodel$=512 to a larger dimension $dff$=2048, and the second layer projects it back to $dmodel$.\n2. **Activation Function**:\n   <br>\n   A Rectified Linear Unit (ReLU) activation function is applied between these two linear layers. This function is defined as ReLU(x)=max(0,x) and is used to introduce non-linearity into the model, helping it to learn more complex patterns.\n3. **Position-wise Processing**:\n   <br>\n   Despite the sequential nature of the input data, each position (i.e., each word’s representation in a sentence) is processed independently with the same FFN. This is akin to applying the same transformation across all positions, ensuring uniformity in extracting features from different parts of the input sequence.\n\n### Mathematical Representation:\n$$\nFFN(X) = Activation(0, xW_1 + b_1)W_2 + b_2\n$$\n\nHere, Activation represents the non-linear activation function, W₁ and W₂ are weight matrices, and b₁ and b₂ are bias vectors. The presence of the activation function allows FFN to break the linearity and alter the input’s distribution and topological structure.\n</span>","metadata":{}},{"cell_type":"code","source":"#Implementation of FeedForward\nclass FeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout=0.1): \n        #d_ff: hidden layer dimension \n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff) #input layer\n        self.linear2 = nn.Linear(d_ff, d_model) #hidden layer\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T01:36:26.923756Z","iopub.execute_input":"2025-12-19T01:36:26.924332Z","iopub.status.idle":"2025-12-19T01:36:26.929158Z","shell.execute_reply.started":"2025-12-19T01:36:26.924308Z","shell.execute_reply":"2025-12-19T01:36:26.928286Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## 1.4 Create Encoder Layer\n\n<span style=\"font-size:18px;\">\n\n### Encoder Architechture in Transformers:\n<img src=\"https://pytorch.org/wp-content/uploads/2024/11/2022-7-12-a-better-transformer-for-fast-transformer-encoder-inference-1.png\" style=\"display: block; margin: 0 auto;\" width=\"600\">\n</span>","metadata":{}},{"cell_type":"code","source":"#Implementation of Encoder Layer\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model: int, num_heads: int , d_ff: int, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.ff = FeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.ff(x)\n        return self.norm2(x + self.dropout(ff_output))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T01:36:29.233573Z","iopub.execute_input":"2025-12-19T01:36:29.234354Z","iopub.status.idle":"2025-12-19T01:36:29.239483Z","shell.execute_reply.started":"2025-12-19T01:36:29.234329Z","shell.execute_reply":"2025-12-19T01:36:29.238786Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 1.5 Create Decoder Layer\n\n<span style=\"font-size:18px;\">\n\n### Decoder Architechture:\n<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/0*uVD7vobj-o1tU-MP.png\" style=\"display: block; margin: 0 auto;\" width=\"600\">\n\n</span>","metadata":{}},{"cell_type":"code","source":"#Implementation of Decoder Layer\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout=0.1):\n        super().__init__()\n        self.masked_attn = MultiHeadAttention(d_model, num_heads)\n        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n        attn_output = self.masked_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        attn_output = self.enc_dec_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        ff_output = self.ff(x)\n        return self.norm3(x + self.dropout(ff_output))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T01:36:32.482879Z","iopub.execute_input":"2025-12-19T01:36:32.483455Z","iopub.status.idle":"2025-12-19T01:36:32.489468Z","shell.execute_reply.started":"2025-12-19T01:36:32.483431Z","shell.execute_reply":"2025-12-19T01:36:32.488726Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# 2. Create Transformers\n<span style=\"font-size:18px\">\n    \n## 2.1 The Padding Mask:\nThe Padding mask could seem trivial at first sight, but it has its own quibbles. First reason on why it is necessary: Not all the sentences have the same lenght!\n<br><br>\nWe:\n\n- Add Padding tokens to bring all the sentences to have the same lenght;\n- Create a mask that is able to block the softmax function to consider these uninformative tokens.\n\n**What is the shape of the Padding Mask?**\nFirst, if we want to talk about padding mask we need to consider the Batch size > 1 that we’ll name B. Hence, Q ∈ R^{B × L × E}, K ∈ R^{B × L × E}, V ∈ R^{B × L × E}, L is the sequence length and E is the embedding size.\n\nNow, we’ll use an arbitrary value for the padding token [PAD] , to align all the |B| sequences to the same lenght L .\n\nAs an example, the “proto-padding-mask” where |B| = 4 and |L| = 6 , will be:\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*hzXXmCShuznJz0ixirWUTg.png\" style=\"display: block; margin: 0 auto;\" width=\"600\">\n<br> <br>\nRemember that the scaled-dot-product attention function that works with a generic mask is:\n<br> <br>\n<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*x1ZxEXba7MQ1A4wQQTnW1Q.png\" style=\"display: block; margin: 0 auto;\" width=\"600\">\n<br><br>\nfor the operation QK^{T} the transposition for the tensor K is done only on the last two dimensions (the batch dim is not considered), so:\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*PgEHfQHa-Xz7cKm9qsDDPQ.png\" style=\"display: block; margin: 0 auto;\" width=\"600\">\n<br><br>\nNow, for each sentence in the set of size | B | we have a L × L matrix that should be masked. To better understand how to construct our padding mask we can make and example with a single sentence, let’s say the third row!\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*I7cV1G1QPT-HO0SvLohsBw.png\" style=\"display: block; margin: 0 auto;\" width=\"600\">\n<br><br>\nConsidering every element like x_7 ∈ R^{E} . So,\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*EltJK6FwLh0PtyTXZxUYag.png\" style=\"display: block; margin: 0 auto;\" width=\"600\">\n<br><br>\nIt’s easy to see that every position in which we have a multiplication by the padding token (actually a dot product because every entry is ∈ R^{E} ) should be masked because is uninformative.\n\nHence, our padding mask for the third sentence will be:\n<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*ieXRW2lU96ivyhs9YBPmIg.png\" style=\"display: block; margin: 0 auto;\" width=\"600\">\n</span>","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Look Ahead Mask:\n<span style=\"font-size:18px\">\nThe look-ahead mask was originally used in the Attention is All You Need paper for the original transformer. The look-ahead mask is used so that the model can be trained on an entire sequence of text at once as opposed to training the model one word at a time. The original transformer model is what’s called an autoregressive model. This means it predicts using only data from the past. The original transformer was made for translation, so this type of model makes sense. When predicting the translated sentence, the model will predict words one at a time. Say I had a sentence:\n\n“How are you”\n\nThe model would translate the sentence to Spanish one word at a time:\n\nPrediction 1: Given “”, the model predicts the next word is “cómo”\n\nPrediction 2: Given “cómo”, the model predicts the next word is “estás”\n\nPrediction 3: Given “cómo estás” the model predicts the next word is “<END>” signifying the end of the sequence\n\nWhat if we wanted the model to learn this translation? Then we could feed it one word at a time, resulting in three predictions from the model. This process is very slow as it requires S (the sequence length) inferences from the model to get a single sentence translation prediction from the model. Instead, we feed it the whole sentence “cómo estás <END> …” and use a clever masking trick so the model cannot look ahead at future tokens, only past tokens. This way it requires a single inference step to get an entire sentence translation from the model.\n\nThe formula for self-attention with a look-ahead mask is the same as the padding mask. The only change has to do with the mask itself.\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Yh4jMeqVpf7KlD_aBHO62A.png\" style=\"display: block; margin: 0 auto;\" width=\"600\">\n<br><br>\n\nThe mask has a triangle of -∞ in the upper right and 0s elsewhere. Let’s see how this affects the softmax of the weight matrix.\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*G1UoOVV3F7iTLYr547xU6A.png\" style=\"display: block; margin: 0 auto;\" width=\"600\">\nThe weight matrix has some interesting results. The first-row aQ is only weighted by itself aᴷ. Since a is the first token in the sequence, it should not be affected by any other token in the sequence as none of the other tokens exist yet.\n\nOn the second row, b is affected by both a and b. Since b is the second token, it should only be affected by the first token, a.\n\nIn the last row, the last token in the sequence, D, is affected by all other tokens as the last token in the sequence should have context of all other tokens in the sequence.\n\nFinally, let’s see how the mask affects the output of the attention function.\n<br><br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Dcy934GlldVKQcbOPL_yKw.png\" style=\"display: block; margin: 0 auto;\" width=\"600\">\n<br><br>\nSimilar to the weight matrix, the resulting vectors are only affected by the tokens preceding the token represented in that vector. The new token embedding of a is in the first row of the resulting vector. Since this token only has context of itself, it will only be a combination of itself.\n\nThe second token b has context of a, so the resulting vector is a combination of a and b.\n\nThe last token D has context of all other tokens, so the resulting vector is a combination of all other tokens.\n\n</span>\n","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, d_model: int = 512, num_heads: int = 8,\n                num_layers: int = 6, d_ff: int = 2048, max_len: int = 5000, dropout: float = 0.1):\n        super().__init__()\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n\n        self.final_linear = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def create_padding_mask(self, seq, pad_token=0):\n        return (seq != pad_token).unsqueeze(1).unsqueeze(2)\n\n    def create_look_ahead_mask(self, seq_len):\n        mask = (torch.triu(torch.ones(seq_len, seq_len), diagonal=1)).bool()\n        return ~mask\n\n    def forward(self, src, tgt, src_pad_mask=None, tgt_pad_mask=None):\n        if src_pad_mask is None:\n            src_pad_mask = self.create_padding_mask(src)\n        if tgt_pad_mask is None:\n            tgt_pad_mask = self.create_padding_mask(tgt)\n\n        tgt_look_ahead_mask = self.create_look_ahead_mask(tgt.size(1)).to(tgt.device)\n        tgt_mask = tgt_pad_mask & tgt_look_ahead_mask\n\n        src_emb = self.dropout(self.pos_encoding(self.src_embedding(src) * math.sqrt(self.src_embedding.embedding_dim)))\n        tgt_emb = self.dropout(self.pos_encoding(self.tgt_embedding(tgt) * math.sqrt(self.tgt_embedding.embedding_dim)))\n\n        enc_output = src_emb\n        for layer in self.encoder_layers:\n            enc_output = layer(enc_output, src_pad_mask)\n\n        dec_output = tgt_emb\n        for layer in self.decoder_layers:\n            dec_output = layer(dec_output, enc_output, src_pad_mask, tgt_mask)\n\n        return self.final_linear(dec_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T01:36:37.132695Z","iopub.execute_input":"2025-12-19T01:36:37.133455Z","iopub.status.idle":"2025-12-19T01:36:37.143222Z","shell.execute_reply.started":"2025-12-19T01:36:37.133413Z","shell.execute_reply":"2025-12-19T01:36:37.142234Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# 3. Create Dataset\n\n<span style=\"font-size:18px\">\nFor the training dataset, I'll use the ncduy/mt-en-vi for training a seq2seq transformers.\n</span>","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"ncduy/mt-en-vi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T01:36:40.201347Z","iopub.execute_input":"2025-12-19T01:36:40.201720Z","iopub.status.idle":"2025-12-19T01:37:05.404155Z","shell.execute_reply.started":"2025-12-19T01:36:40.201693Z","shell.execute_reply":"2025-12-19T01:37:05.403532Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3274a740432547f5be0efcc7683ecd89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/597M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f6bd50547f3457f80c3fdc5f9613831"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"valid.csv:   0%|          | 0.00/2.45M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2784df4a6b64389ae41c9851419b113"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/2.43M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2a50e3d79924a83b9aa9ffd7ed9378c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2884451 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c2a69a34bae43b58a321fdf4f617598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11316 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d66c5fd30dcf4fb0980d556bab93ac1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11225 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3e8338a96434819a1a38204ff6d69aa"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train = ds[\"train\"].select(range(10000))\ntrain['en'][:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T01:37:10.109095Z","iopub.execute_input":"2025-12-19T01:37:10.110006Z","iopub.status.idle":"2025-12-19T01:37:10.118913Z","shell.execute_reply.started":"2025-12-19T01:37:10.109978Z","shell.execute_reply":"2025-12-19T01:37:10.118188Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[\"- Sorry, that question's not on here.\",\n 'He wants you to come with him immediately.',\n 'I thought we could use some company.',\n 'It was founded in 2008 by this anonymous programmer using a pseudonym Satoshi Nakamoto.',\n 'With both of these methods, no two prints are exactly alike, but both reveal dramatic images of the fish.']"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"src_texts = []\nfor text in train['en'][:10000]:\n    src_texts.append(text) \n\ntgt_texts = []\nfor text in train['vi'][:10000]:\n    tgt_texts.append(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T01:37:12.655239Z","iopub.execute_input":"2025-12-19T01:37:12.655759Z","iopub.status.idle":"2025-12-19T01:37:12.693991Z","shell.execute_reply.started":"2025-12-19T01:37:12.655736Z","shell.execute_reply":"2025-12-19T01:37:12.693327Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\n\nBOS_TOKEN = \"<bos>\"\nEOS_TOKEN = \"<eos>\"\nPAD_TOKEN = \"<pad>\"\nUNK_TOKEN = \"<unk>\"\n\n#Build vocab from the dataset\ndef build_vocab(texts, min_freq=1):\n    vocab = {PAD_TOKEN: 0, BOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n    idx = 4\n    for text in texts:\n        for word in text.lower().split():\n            if word not in vocab:\n                vocab[word] = idx\n                idx += 1\n    return vocab\n\nsrc_vocab = build_vocab(src_texts) #English vocab\ntgt_vocab = build_vocab(tgt_texts) #Vietnamese vocab\n\nsrc_vocab_size = len(src_vocab)\ntgt_vocab_size = len(tgt_vocab)\n\ndef text_to_indices(text, vocab, is_src=True):\n''' E.g: src_texts = [\"- Sorry, that question's not on here.\",\n                        'He wants you to come with him immediately.',\n                        'I thought we could use some company.',\n                        'It was founded in 2008 by this anonymous programmer using a pseudonym Satoshi Nakamoto.',\n                        'With both of these methods, no two prints are exactly alike, but both reveal dramatic images of the fish.']\n    \n    Output (conceptual):\n    [BOS, idx1, idx2, ..., idxN, EOS]                   \n'''\n    words = text.lower().split() if is_src else text.split()\n    return [vocab.get(BOS_TOKEN, 1)] + [vocab.get(w, vocab[UNK_TOKEN]) for w in words] + [vocab.get(EOS_TOKEN, 2)]\n\n\n\nclass TranslationDataset(Dataset):\n    def __init__(self, src_texts, tgt_texts):\n        self.src = [torch.tensor(text_to_indices(s, src_vocab)) for s in src_texts]\n        self.tgt = [torch.tensor(text_to_indices(t, tgt_vocab, is_src=False)) for t in tgt_texts]\n    \n    def __len__(self):\n        return len(self.src)\n    \n    def __getitem__(self, idx):\n        return self.src[idx], self.tgt[idx]\n\ndataset = TranslationDataset(src_texts, tgt_texts)\n\ndef collate_fn(batch):\n    src_batch, tgt_batch = zip(*batch)\n    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, padding_value=src_vocab[PAD_TOKEN], batch_first=True)\n    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_batch, padding_value=tgt_vocab[PAD_TOKEN], batch_first=True)\n\n    tgt_input = tgt_padded[:, :-1]\n    tgt_target = tgt_padded[:, 1:]\n    return src_padded, tgt_input, tgt_target\n\ndataloader = DataLoader(\n    dataset,\n    batch_size=2,\n    collate_fn=collate_fn,\n    shuffle=True,\n    pin_memory=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T01:48:56.691004Z","iopub.execute_input":"2025-12-19T01:48:56.691622Z","iopub.status.idle":"2025-12-19T01:48:57.181860Z","shell.execute_reply.started":"2025-12-19T01:48:56.691575Z","shell.execute_reply":"2025-12-19T01:48:57.181050Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nmodel = Transformer(\n    src_vocab_size=src_vocab_size,\n    tgt_vocab_size=tgt_vocab_size,\n    d_model=128,\n    num_heads=4,\n    num_layers=2,\n    d_ff=256,\n    max_len=512,\n    dropout=0.1\n).to(device)\n\nmodel.train()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab[PAD_TOKEN])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T01:48:58.642191Z","iopub.execute_input":"2025-12-19T01:48:58.642770Z","iopub.status.idle":"2025-12-19T01:48:58.728315Z","shell.execute_reply.started":"2025-12-19T01:48:58.642747Z","shell.execute_reply":"2025-12-19T01:48:58.727717Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"class Config():\n    def __init__(self,\n                model: nn.Module,\n                crtiterion: nn.Module,\n                optimizer: optim.Optimizer,\n                epochs: int = 10,\n                tgt_vocab_size: int = 1000):\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.epochs = epochs\n        self.tgt_vocab_size = tgt_vocab_size\n\n    def __call__(self, dataloader: DataLoader):\n        return self.train(dataloader)\n\n    def train(self, dataloader: DataLoader):\n        for epoch in range(self.epochs):\n            total_loss = 0\n            for src, tgt_input, tgt_target in dataloader:\n                optimizer.zero_grad()\n                \n                output = self.model(src, tgt_input) \n                \n                loss = self.criterion(\n                    output.reshape(-1, self.tgt_vocab_size),\n                    tgt_target.reshape(-1)\n                )\n                \n                loss.backward()\n                self.optimizer.step()\n                \n                total_loss += loss.item()\n            \n            avg_loss = total_loss / len(dataloader)\n            print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {avg_loss:.4f}\")\n            \n        return {\"final_loss\": avg_loss}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(epochs):\n    total_loss = 0\n\n    for step, (src, tgt_input, tgt_target) in enumerate(dataloader):\n\n        src = src.to(device)\n        tgt_input = tgt_input.to(device)\n        tgt_target = tgt_target.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(src, tgt_input)\n\n        loss = criterion(\n            output.reshape(-1, tgt_vocab_size),\n            tgt_target.reshape(-1)\n        )\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    \n    print(f\"Epoch [{epoch+1}/{epochs}] \"\n          f\"Last Step Loss: {loss.item():.4f} | \"\n          f\"Avg Loss: {total_loss / len(dataloader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T02:01:16.156508Z","iopub.execute_input":"2025-12-19T02:01:16.157257Z","iopub.status.idle":"2025-12-19T02:09:04.263604Z","shell.execute_reply.started":"2025-12-19T02:01:16.157234Z","shell.execute_reply":"2025-12-19T02:09:04.262786Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/5] Last Step Loss: 4.5108 | Avg Loss: 4.2390\nEpoch [2/5] Last Step Loss: 3.2525 | Avg Loss: 4.0939\nEpoch [3/5] Last Step Loss: 4.7037 | Avg Loss: 3.9689\nEpoch [4/5] Last Step Loss: 4.1405 | Avg Loss: 3.8627\nEpoch [5/5] Last Step Loss: 3.9953 | Avg Loss: 3.7594\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def translate(sentence, max_len=50):\n    model.eval()\n    device = next(model.parameters()).device\n\n    sentence = sentence.lower()\n\n    src_indices = torch.tensor(\n        [text_to_indices(sentence, src_vocab, is_src=True)],\n        device=device\n    )\n\n    tgt_indices = torch.tensor(\n        [[tgt_vocab[BOS_TOKEN]]],\n        device=device\n    )\n\n    with torch.no_grad():\n        for _ in range(max_len):\n            output = model(src_indices, tgt_indices)\n\n            next_token = output[:, -1, :].argmax(dim=-1)\n\n            tgt_indices = torch.cat(\n                [tgt_indices, next_token.unsqueeze(1)],\n                dim=1\n            )\n\n            if next_token.item() == tgt_vocab[EOS_TOKEN]:\n                break\n\n    inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n    words = [\n        inv_tgt_vocab.get(i.item(), \"\")\n        for i in tgt_indices[0, 1:]\n        if i.item() != tgt_vocab[EOS_TOKEN]\n    ]\n\n    return \" \".join(words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T02:00:15.315506Z","iopub.execute_input":"2025-12-19T02:00:15.316343Z","iopub.status.idle":"2025-12-19T02:00:15.323314Z","shell.execute_reply.started":"2025-12-19T02:00:15.316309Z","shell.execute_reply":"2025-12-19T02:00:15.322586Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print(translate(\"He wants you to come with him immediately.\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T02:09:10.548175Z","iopub.execute_input":"2025-12-19T02:09:10.548438Z","iopub.status.idle":"2025-12-19T02:09:10.628902Z","shell.execute_reply.started":"2025-12-19T02:09:10.548421Z","shell.execute_reply":"2025-12-19T02:09:10.628156Z"}},"outputs":[{"name":"stdout","text":"<unk> có thể làm gì với chúng ta có thể làm sao không.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"torch.save({\n    \"model\": model.state_dict(),\n    \"src_vocab\": src_vocab,\n    \"tgt_vocab\": tgt_vocab\n}, \"transformer_translation.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T02:09:48.603026Z","iopub.execute_input":"2025-12-19T02:09:48.603322Z","iopub.status.idle":"2025-12-19T02:09:48.710296Z","shell.execute_reply.started":"2025-12-19T02:09:48.603304Z","shell.execute_reply":"2025-12-19T02:09:48.709506Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}